{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harkum\\AppData\\Local\\Temp\\ipykernel_18036\\310523901.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('covid19_tweets.csv',encoding = \"ISO-8859-1\", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_created</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_friends</th>\n",
       "      <th>user_favourites</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>source</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>áá¥â»Õ¬êÏ®</td>\n",
       "      <td>astroworld</td>\n",
       "      <td>wednesday addams as a disney princess keepin i...</td>\n",
       "      <td>2017-05-26 05:46:42</td>\n",
       "      <td>624</td>\n",
       "      <td>950</td>\n",
       "      <td>18775</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-07-25 12:27:21</td>\n",
       "      <td>If I smelled the scent of hand sanitizers toda...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tom Basile ðºð¸</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Husband, Father, Columnist &amp; Commentator. Auth...</td>\n",
       "      <td>2009-04-16 20:06:23</td>\n",
       "      <td>2253</td>\n",
       "      <td>1677</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-07-25 12:27:17</td>\n",
       "      <td>Hey @Yankees @YankeesPR and @MLB - wouldn't it...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Time4fisticuffs</td>\n",
       "      <td>Pewee Valley, KY</td>\n",
       "      <td>#Christian #Catholic #Conservative #Reagan #Re...</td>\n",
       "      <td>2009-02-28 18:57:41</td>\n",
       "      <td>9275</td>\n",
       "      <td>9525</td>\n",
       "      <td>7254</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-07-25 12:27:14</td>\n",
       "      <td>@diane3443 @wdunlap @realDonaldTrump Trump nev...</td>\n",
       "      <td>['COVID19']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ethel mertz</td>\n",
       "      <td>Stuck in the Middle</td>\n",
       "      <td>#Browns #Indians #ClevelandProud #[]_[] #Cavs ...</td>\n",
       "      <td>2019-03-07 01:45:06</td>\n",
       "      <td>197</td>\n",
       "      <td>987</td>\n",
       "      <td>1488</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-07-25 12:27:10</td>\n",
       "      <td>@brookbanktv The one gift #COVID19 has give me...</td>\n",
       "      <td>['COVID19']</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DIPR-J&amp;K</td>\n",
       "      <td>Jammu and Kashmir</td>\n",
       "      <td>ðï¸Official Twitter handle of Department o...</td>\n",
       "      <td>2017-02-12 06:45:15</td>\n",
       "      <td>101009</td>\n",
       "      <td>168</td>\n",
       "      <td>101</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-07-25 12:27:08</td>\n",
       "      <td>25 July : Media Bulletin on Novel #CoronaVirus...</td>\n",
       "      <td>['CoronaVirusUpdates', 'COVID19']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_name         user_location  \\\n",
       "0     áá¥â»Õ¬ê\n",
       "Ï®            astroworld   \n",
       "1  Tom Basile ðºð¸          New York, NY   \n",
       "2      Time4fisticuffs      Pewee Valley, KY   \n",
       "3          ethel mertz  Stuck in the Middle    \n",
       "4             DIPR-J&K     Jammu and Kashmir   \n",
       "\n",
       "                                    user_description         user_created  \\\n",
       "0  wednesday addams as a disney princess keepin i...  2017-05-26 05:46:42   \n",
       "1  Husband, Father, Columnist & Commentator. Auth...  2009-04-16 20:06:23   \n",
       "2  #Christian #Catholic #Conservative #Reagan #Re...  2009-02-28 18:57:41   \n",
       "3  #Browns #Indians #ClevelandProud #[]_[] #Cavs ...  2019-03-07 01:45:06   \n",
       "4  ðï¸Official Twitter handle of Department o...  2017-02-12 06:45:15   \n",
       "\n",
       "   user_followers  user_friends  user_favourites  user_verified  \\\n",
       "0             624           950            18775          False   \n",
       "1            2253          1677               24           True   \n",
       "2            9275          9525             7254          False   \n",
       "3             197           987             1488          False   \n",
       "4          101009           168              101          False   \n",
       "\n",
       "                  date                                               text  \\\n",
       "0  2020-07-25 12:27:21  If I smelled the scent of hand sanitizers toda...   \n",
       "1  2020-07-25 12:27:17  Hey @Yankees @YankeesPR and @MLB - wouldn't it...   \n",
       "2  2020-07-25 12:27:14  @diane3443 @wdunlap @realDonaldTrump Trump nev...   \n",
       "3  2020-07-25 12:27:10  @brookbanktv The one gift #COVID19 has give me...   \n",
       "4  2020-07-25 12:27:08  25 July : Media Bulletin on Novel #CoronaVirus...   \n",
       "\n",
       "                            hashtags               source  is_retweet  \n",
       "0                                NaN   Twitter for iPhone       False  \n",
       "1                                NaN  Twitter for Android       False  \n",
       "2                        ['COVID19']  Twitter for Android       False  \n",
       "3                        ['COVID19']   Twitter for iPhone       False  \n",
       "4  ['CoronaVirusUpdates', 'COVID19']  Twitter for Android       False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_name', 'user_location', 'user_description', 'user_created',\n",
       "       'user_followers', 'user_friends', 'user_favourites', 'user_verified',\n",
       "       'date', 'text', 'hashtags', 'source', 'is_retweet'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179108, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, \"['COVID19']\", \"['CoronaVirusUpdates', 'COVID19']\", ...,\n",
       "       \"['COVID19', 'coronavirus', 'TrumpVirus']\",\n",
       "       \"['COVID19', 'CloseTheSchools', 'KeepTheSchoolsClosed', 'KeepTeachersAlive', 'K12']\",\n",
       "       \"['nurses', 'COVID19', 'coronavirus', 'schools']\"], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['hashtags'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Text Cleaning Package\n",
    "import neattext.functions as nfx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BTC_ADDRESS_REGEX',\n",
       " 'CURRENCY_REGEX',\n",
       " 'CURRENCY_SYMB_REGEX',\n",
       " 'Counter',\n",
       " 'DATE_REGEX',\n",
       " 'EMAIL_REGEX',\n",
       " 'EMOJI_REGEX',\n",
       " 'HASTAG_REGEX',\n",
       " 'MASTERCard_REGEX',\n",
       " 'MD5_SHA_REGEX',\n",
       " 'MOST_COMMON_PUNCT_REGEX',\n",
       " 'NUMBERS_REGEX',\n",
       " 'PHONE_REGEX',\n",
       " 'PoBOX_REGEX',\n",
       " 'SPECIAL_CHARACTERS_REGEX',\n",
       " 'STOPWORDS',\n",
       " 'STOPWORDS_de',\n",
       " 'STOPWORDS_en',\n",
       " 'STOPWORDS_es',\n",
       " 'STOPWORDS_fr',\n",
       " 'STOPWORDS_ru',\n",
       " 'STOPWORDS_yo',\n",
       " 'STREET_ADDRESS_REGEX',\n",
       " 'TextFrame',\n",
       " 'URL_PATTERN',\n",
       " 'USER_HANDLES_REGEX',\n",
       " 'VISACard_REGEX',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__generate_text',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__numbers_dict',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_lex_richness_herdan',\n",
       " '_lex_richness_maas_ttr',\n",
       " 'clean_text',\n",
       " 'defaultdict',\n",
       " 'digit2words',\n",
       " 'extract_btc_address',\n",
       " 'extract_currencies',\n",
       " 'extract_currency_symbols',\n",
       " 'extract_dates',\n",
       " 'extract_emails',\n",
       " 'extract_emojis',\n",
       " 'extract_hashtags',\n",
       " 'extract_html_tags',\n",
       " 'extract_mastercard_addr',\n",
       " 'extract_md5sha',\n",
       " 'extract_numbers',\n",
       " 'extract_pattern',\n",
       " 'extract_phone_numbers',\n",
       " 'extract_postoffice_box',\n",
       " 'extract_shortwords',\n",
       " 'extract_special_characters',\n",
       " 'extract_stopwords',\n",
       " 'extract_street_address',\n",
       " 'extract_terms_in_bracket',\n",
       " 'extract_urls',\n",
       " 'extract_userhandles',\n",
       " 'extract_visacard_addr',\n",
       " 'fix_contractions',\n",
       " 'generate_sentence',\n",
       " 'hamming_distance',\n",
       " 'inverse_df',\n",
       " 'lexical_richness',\n",
       " 'markov_chain',\n",
       " 'math',\n",
       " 'nlargest',\n",
       " 'normalize',\n",
       " 'num2words',\n",
       " 'random',\n",
       " 're',\n",
       " 'read_txt',\n",
       " 'remove_accents',\n",
       " 'remove_bad_quotes',\n",
       " 'remove_btc_address',\n",
       " 'remove_currencies',\n",
       " 'remove_currency_symbols',\n",
       " 'remove_custom_pattern',\n",
       " 'remove_custom_words',\n",
       " 'remove_dates',\n",
       " 'remove_emails',\n",
       " 'remove_emojis',\n",
       " 'remove_hashtags',\n",
       " 'remove_html_tags',\n",
       " 'remove_mastercard_addr',\n",
       " 'remove_md5sha',\n",
       " 'remove_multiple_spaces',\n",
       " 'remove_non_ascii',\n",
       " 'remove_numbers',\n",
       " 'remove_phone_numbers',\n",
       " 'remove_postoffice_box',\n",
       " 'remove_puncts',\n",
       " 'remove_punctuations',\n",
       " 'remove_shortwords',\n",
       " 'remove_special_characters',\n",
       " 'remove_stopwords',\n",
       " 'remove_street_address',\n",
       " 'remove_terms_in_bracket',\n",
       " 'remove_urls',\n",
       " 'remove_userhandles',\n",
       " 'remove_visacard_addr',\n",
       " 'replace_bad_quotes',\n",
       " 'replace_currencies',\n",
       " 'replace_currency_symbols',\n",
       " 'replace_dates',\n",
       " 'replace_emails',\n",
       " 'replace_emojis',\n",
       " 'replace_numbers',\n",
       " 'replace_phone_numbers',\n",
       " 'replace_special_characters',\n",
       " 'replace_term',\n",
       " 'replace_urls',\n",
       " 'string',\n",
       " 'term_freq',\n",
       " 'to_txt',\n",
       " 'unicodedata',\n",
       " 'word_freq',\n",
       " 'word_length_freq']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n'BTC_ADDRESS_REGEX',\\n 'CURRENCY_REGEX',\\n 'CURRENCY_SYMB_REGEX',\\n 'Counter',\\n 'DATE_REGEX',\\n 'EMAIL_REGEX',\\n 'EMOJI_REGEX',\\n 'HASTAG_REGEX',\\n 'MASTERCard_REGEX',\\n 'MD5_SHA_REGEX',\\n 'MOST_COMMON_PUNCT_REGEX',\\n 'NUMBERS_REGEX',\\n 'PHONE_REGEX',\\n 'PoBOX_REGEX',\\n 'SPECIAL_CHARACTERS_REGEX',\\n 'STOPWORDS',\\n 'STOPWORDS_de',\\n 'STOPWORDS_en',\\n 'STOPWORDS_es',\\n 'STOPWORDS_fr',\\n 'STOPWORDS_ru',\\n 'STOPWORDS_yo',\\n 'STREET_ADDRESS_REGEX',\\n 'TextFrame',\\n 'URL_PATTERN',\\n 'USER_HANDLES_REGEX',\\n 'VISACard_REGEX',\\n '__builtins__',\\n '__cached__',\\n '__doc__',\\n '__file__',\\n '__generate_text',\\n '__loader__',\\n '__name__',\\n '__numbers_dict',\\n '__package__',\\n '__spec__',\\n '_lex_richness_herdan',\\n '_lex_richness_maas_ttr',\\n 'clean_text',\\n 'defaultdict',\\n 'digit2words',\\n 'extract_btc_address',\\n 'extract_currencies',\\n 'extract_currency_symbols',\\n 'extract_dates',\\n 'extract_emails',\\n 'extract_emojis',\\n 'extract_hashtags',\\n 'extract_html_tags',\\n 'extract_mastercard_addr',\\n 'extract_md5sha',\\n 'extract_numbers',\\n 'extract_pattern',\\n 'extract_phone_numbers',\\n 'extract_postoffice_box',\\n 'extract_shortwords',\\n 'extract_special_characters',\\n 'extract_stopwords',\\n 'extract_street_address',\\n 'extract_urls',\\n 'extract_userhandles',\\n 'extract_visacard_addr',\\n 'fix_contractions',\\n 'generate_sentence',\\n 'hamming_distance',\\n 'inverse_df',\\n 'lexical_richness',\\n 'markov_chain',\\n 'math',\\n 'nlargest',\\n 'normalize',\\n 'num2words',\\n 'random',\\n 're',\\n 'read_txt',\\n 'remove_bad_quotes',\\n 'remove_btc_address',\\n 'remove_currencies',\\n 'remove_currency_symbols',\\n 'remove_custom_pattern',\\n 'remove_custom_words',\\n 'remove_dates',\\n 'remove_emails',\\n 'remove_emojis',\\n 'remove_hashtags',\\n 'remove_html_tags',\\n 'remove_mastercard_addr',\\n 'remove_md5sha',\\n 'remove_multiple_spaces',\\n 'remove_non_ascii',\\n 'remove_numbers',\\n 'remove_phone_numbers',\\n 'remove_postoffice_box',\\n 'remove_puncts',\\n 'remove_punctuations',\\n 'remove_shortwords',\\n 'remove_special_characters',\\n 'remove_stopwords',\\n 'remove_street_address',\\n 'remove_urls',\\n 'remove_userhandles',\\n 'remove_visacard_addr',\\n 'replace_bad_quotes',\\n 'replace_currencies',\\n 'replace_currency_symbols',\\n 'replace_dates',\\n 'replace_emails',\\n 'replace_emojis',\\n 'replace_numbers',\\n 'replace_phone_numbers',\\n 'replace_special_characters',\\n 'replace_term',\\n 'replace_urls',\\n 'string',\\n 'term_freq',\\n 'to_txt',\\n 'word_freq',\\n 'word_length_freq'\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "'BTC_ADDRESS_REGEX',\n",
    " 'CURRENCY_REGEX',\n",
    " 'CURRENCY_SYMB_REGEX',\n",
    " 'Counter',\n",
    " 'DATE_REGEX',\n",
    " 'EMAIL_REGEX',\n",
    " 'EMOJI_REGEX',\n",
    " 'HASTAG_REGEX',\n",
    " 'MASTERCard_REGEX',\n",
    " 'MD5_SHA_REGEX',\n",
    " 'MOST_COMMON_PUNCT_REGEX',\n",
    " 'NUMBERS_REGEX',\n",
    " 'PHONE_REGEX',\n",
    " 'PoBOX_REGEX',\n",
    " 'SPECIAL_CHARACTERS_REGEX',\n",
    " 'STOPWORDS',\n",
    " 'STOPWORDS_de',\n",
    " 'STOPWORDS_en',\n",
    " 'STOPWORDS_es',\n",
    " 'STOPWORDS_fr',\n",
    " 'STOPWORDS_ru',\n",
    " 'STOPWORDS_yo',\n",
    " 'STREET_ADDRESS_REGEX',\n",
    " 'TextFrame',\n",
    " 'URL_PATTERN',\n",
    " 'USER_HANDLES_REGEX',\n",
    " 'VISACard_REGEX',\n",
    " '__builtins__',\n",
    " '__cached__',\n",
    " '__doc__',\n",
    " '__file__',\n",
    " '__generate_text',\n",
    " '__loader__',\n",
    " '__name__',\n",
    " '__numbers_dict',\n",
    " '__package__',\n",
    " '__spec__',\n",
    " '_lex_richness_herdan',\n",
    " '_lex_richness_maas_ttr',\n",
    " 'clean_text',\n",
    " 'defaultdict',\n",
    " 'digit2words',\n",
    " 'extract_btc_address',\n",
    " 'extract_currencies',\n",
    " 'extract_currency_symbols',\n",
    " 'extract_dates',\n",
    " 'extract_emails',\n",
    " 'extract_emojis',\n",
    " 'extract_hashtags',\n",
    " 'extract_html_tags',\n",
    " 'extract_mastercard_addr',\n",
    " 'extract_md5sha',\n",
    " 'extract_numbers',\n",
    " 'extract_pattern',\n",
    " 'extract_phone_numbers',\n",
    " 'extract_postoffice_box',\n",
    " 'extract_shortwords',\n",
    " 'extract_special_characters',\n",
    " 'extract_stopwords',\n",
    " 'extract_street_address',\n",
    " 'extract_urls',\n",
    " 'extract_userhandles',\n",
    " 'extract_visacard_addr',\n",
    " 'fix_contractions',\n",
    " 'generate_sentence',\n",
    " 'hamming_distance',\n",
    " 'inverse_df',\n",
    " 'lexical_richness',\n",
    " 'markov_chain',\n",
    " 'math',\n",
    " 'nlargest',\n",
    " 'normalize',\n",
    " 'num2words',\n",
    " 'random',\n",
    " 're',\n",
    " 'read_txt',\n",
    " 'remove_bad_quotes',\n",
    " 'remove_btc_address',\n",
    " 'remove_currencies',\n",
    " 'remove_currency_symbols',\n",
    " 'remove_custom_pattern',\n",
    " 'remove_custom_words',\n",
    " 'remove_dates',\n",
    " 'remove_emails',\n",
    " 'remove_emojis',\n",
    " 'remove_hashtags',\n",
    " 'remove_html_tags',\n",
    " 'remove_mastercard_addr',\n",
    " 'remove_md5sha',\n",
    " 'remove_multiple_spaces',\n",
    " 'remove_non_ascii',\n",
    " 'remove_numbers',\n",
    " 'remove_phone_numbers',\n",
    " 'remove_postoffice_box',\n",
    " 'remove_puncts',\n",
    " 'remove_punctuations',\n",
    " 'remove_shortwords',\n",
    " 'remove_special_characters',\n",
    " 'remove_stopwords',\n",
    " 'remove_street_address',\n",
    " 'remove_urls',\n",
    " 'remove_userhandles',\n",
    " 'remove_visacard_addr',\n",
    " 'replace_bad_quotes',\n",
    " 'replace_currencies',\n",
    " 'replace_currency_symbols',\n",
    " 'replace_dates',\n",
    " 'replace_emails',\n",
    " 'replace_emojis',\n",
    " 'replace_numbers',\n",
    " 'replace_phone_numbers',\n",
    " 'replace_special_characters',\n",
    " 'replace_term',\n",
    " 'replace_urls',\n",
    " 'string',\n",
    " 'term_freq',\n",
    " 'to_txt',\n",
    " 'word_freq',\n",
    " 'word_length_freq'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ð\\x9f\\x91\\x8bð\\x9f\\x8f»@PattyHajdu @NavdeepSBains â\\x80\\x94 no one will be safe from #COVID19 until everyone is safe. Will you commit to ensureâ\\x80¦ https://t.co/aWCJo6eKvC'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample tweet \n",
    "data['text'].iloc[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the hast tags from the tweet text \n",
    "data['extracted_hashtags'] = data['text'].apply(nfx.extract_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extracted_hashtags</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[#COVID19]</td>\n",
       "      <td>['COVID19']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[#COVID19]</td>\n",
       "      <td>['COVID19']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[#CoronaVirusUpdates, #COVID19]</td>\n",
       "      <td>['CoronaVirusUpdates', 'COVID19']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179103</th>\n",
       "      <td>[#WearAMask]</td>\n",
       "      <td>['WearAMask']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179104</th>\n",
       "      <td>[#COVID19]</td>\n",
       "      <td>['COVID19']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179105</th>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179106</th>\n",
       "      <td>[#COVID19]</td>\n",
       "      <td>['COVID19']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179107</th>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179108 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     extracted_hashtags                           hashtags\n",
       "0                                    []                                NaN\n",
       "1                                    []                                NaN\n",
       "2                            [#COVID19]                        ['COVID19']\n",
       "3                            [#COVID19]                        ['COVID19']\n",
       "4       [#CoronaVirusUpdates, #COVID19]  ['CoronaVirusUpdates', 'COVID19']\n",
       "...                                 ...                                ...\n",
       "179103                     [#WearAMask]                      ['WearAMask']\n",
       "179104                       [#COVID19]                        ['COVID19']\n",
       "179105                               []                                NaN\n",
       "179106                       [#COVID19]                        ['COVID19']\n",
       "179107                               []                                NaN\n",
       "\n",
       "[179108 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare the hastags with the extracted data hastags \n",
    "data[['extracted_hashtags','hashtags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['clean_tweets'] = data['text'].apply(nfx.remove_hashtags)  # clean hastags\n",
    "data['clean_tweets'] = data['clean_tweets'].apply(lambda x : nfx.remove_userhandles(x))  # remove user handles followed by @\n",
    "data['clean_tweets'] = data['clean_tweets'].apply(nfx.remove_urls)  # remove ursl \n",
    "data['clean_tweets'] = data['clean_tweets'].apply(nfx.remove_multiple_spaces)  # remove multiple spaces \n",
    "data['clean_tweets'] = data['clean_tweets'].apply(nfx.remove_puncts)  # remove punctuations \n",
    "data['clean_tweets'] = data['clean_tweets'].apply(nfx.remove_dates)  # remove dates \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_tweets'] = data['clean_tweets'].apply(nfx.remove_numbers)  # remove numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         []\n",
       "1         []\n",
       "2         []\n",
       "3         []\n",
       "4         []\n",
       "          ..\n",
       "179103    []\n",
       "179104    []\n",
       "179105    []\n",
       "179106    []\n",
       "179107    []\n",
       "Name: text, Length: 179108, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the the emojis with the regular expression\n",
    "data['text'].apply(lambda x : re.findall(r'(\\u00a9|\\u00ae|[\\u2000-\\u3300]|\\ud83c[\\ud000-\\udfff]|\\ud83d[\\ud000-\\udfff]|\\ud83e[\\ud000-\\udfff])',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    Hey and  wouldnt it have made more sense to ha...\n",
       "2     Trump never once claimed was a hoax We all cl...\n",
       "3     The one gift has give me is an appreciation f...\n",
       "4                      July : Media Bulletin on Novel \n",
       "Name: clean_tweets, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.clean_tweets.iloc[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi welcome'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r':','','hi: welcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         []\n",
      "1         []\n",
      "2         []\n",
      "3         []\n",
      "4         []\n",
      "          ..\n",
      "179103    []\n",
      "179104    []\n",
      "179105    []\n",
      "179106    []\n",
      "179107    []\n",
      "Name: text, Length: 179108, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.text.apply(lambda x : re.compile(r'\\S+@\\S+').findall(x))) # find the emails "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hr@Iwillgetbacktoyou@gmail.com',\n",
       " 'business@enquiry@gmail.com',\n",
       " 'review-team@geeksforgeeks.org']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Please send your resumes on Hr@Iwillgetbacktoyou@gmail.com\" \\\n",
    "       \"  for any business enquiry please mail us on business@enquiry@gmail.com\" \\\n",
    "       \" review-team@geeksforgeeks.org \" \\\n",
    "       \"and writing.geeksforgeeks.org\"\n",
    "re.compile(r\"\\S+@\\S+\").findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_tweets'] = data['clean_tweets'].apply(nfx.remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_tweets']=data['clean_tweets'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361\n",
      "{'see', 'mightn', 'upon', 'anyone', 'enough', 'd', 'have', 'and', \"mustn't\", 'keep', 'hundred', 'mostly', 'nowhere', 'was', 'meanwhile', 'off', 'my', 'least', \"wouldn't\", 'whom', 'among', 'beside', 'made', 'eleven', 'namely', 'take', 'whereupon', 'above', 'no', 'became', 'without', 'be', 'amongst', 'two', 'yourself', 'is', 'he', \"you'd\", 'per', 'whose', 'much', 'needn', 'to', \"didn't\", 's', \"aren't\", 'none', 'should', 'since', 'here', 'them', 'first', 'are', 'hereafter', 'within', 'did', 'well', 'make', 'hasn', 'thereupon', 'shan', 'wherein', \"weren't\", 'its', 'say', 'noone', 'otherwise', 'herein', 'll', 'may', 'alone', 'were', \"shan't\", 'me', 'yet', 'former', 'become', 'their', 'therein', 'won', 'over', 'hereby', 'own', 'next', 'what', 'whoever', 'regarding', 'via', 'same', 'anything', 'already', 'theirs', \"doesn't\", 'six', 'five', 'towards', 'along', 'nevertheless', 'before', 'several', 'haven', 'this', 'when', 'show', 'couldn', 'get', \"haven't\", 'your', 'still', \"you'll\", 're', 'cannot', 'from', 'not', 'why', 'whereafter', 'please', 'front', 'else', 'that', 'whenever', 'amount', 'part', 'thru', \"it's\", 'give', 'three', 'perhaps', 'thus', \"should've\", 'y', 't', 'yours', 'might', 'ourselves', 'whereas', 'as', 'one', 'seemed', 'by', 'together', 'thence', 'go', 'many', 'below', 'something', 'empty', 'we', 'mine', 'somehow', 'those', 'until', 'fifteen', 'fifty', 'now', 'formerly', 'thereby', 'while', 'her', 'whether', 'last', 'just', 'up', 'besides', 'serious', 'will', 'ma', 'elsewhere', 'top', 'sixty', 'full', 'never', 'various', 'themselves', 'though', 'm', 'back', 'so', 'of', 'around', 'beyond', 'however', 'yourselves', 'how', 'anywhere', 'such', 'whither', 'could', 'really', 'been', 'wasn', 'onto', 'they', \"she's\", 'ca', 'for', 'whence', 'due', 'himself', 'had', 'seem', 'except', 'others', 'would', \"that'll\", 'us', 'down', 'has', 'most', 'on', 'through', 'quite', 'any', 'having', 'if', 'someone', 'always', 'unless', 'then', 'doing', 'sometimes', 've', 'or', \"isn't\", 'it', 'name', 'isn', 'don', 'move', 'seems', 'becomes', 'seeming', 'she', 'am', 'can', \"you're\", \"you've\", 'in', 'nor', 'hereupon', 'hadn', 'everywhere', 'both', 'hence', 'few', 'put', 'between', 'doesn', 'where', 'beforehand', 'toward', 'afterwards', 'with', 'our', 'being', 'into', 'moreover', 'o', 'a', 'there', 'anyway', 'must', 'but', 'nothing', 'everyone', 'didn', 'even', 'four', 'either', \"couldn't\", \"needn't\", \"don't\", 'against', 'itself', 'an', 'bottom', 'the', 'using', 'all', 'wherever', \"shouldn't\", 'whatever', 'also', 'only', 'often', 'do', 'whole', 'after', 'these', 'you', 'nobody', 'throughout', \"hasn't\", 'latter', 'call', 'does', 'him', 'further', 'wouldn', 'forty', 'hers', 'thereafter', 'ten', 'rather', 'which', 'less', 'than', 'under', 'during', 'aren', 'used', 'ain', 'behind', 'almost', 'weren', 'neither', 'nine', 'out', 'very', 'myself', 'each', 'his', 'shouldn', 'twenty', 'eight', 'at', 'ever', 'about', 'other', 'anyhow', 'twelve', 'somewhere', \"hadn't\", 'third', 'too', 'latterly', \"won't\", 'who', 'every', 'indeed', 'herself', 'more', 'mustn', \"mightn't\", 'i', 'side', 'although', 'across', 'some', 'done', 'therefore', 'everything', 'another', 'ours', 'again', 'becoming', \"wasn't\", 'sometime', 'once', 'whereby', 'because'}\n"
     ]
    }
   ],
   "source": [
    "print(len(nfx.STOPWORDS_en))\n",
    "print(nfx.STOPWORDS_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harkum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')\n",
    "print(len(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_tweets']=data['clean_tweets'].apply(lambda x : nfx.remove_stopwords(x,lang='en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.stem import PorterStemmer\n",
    "port_stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_tweets'] = data['clean_tweets'].apply(lambda x : ' '.join([port_stem.stem(i) for i in x.split()])) # stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct the words \n",
    "# data['clean_tweets'] = data['clean_tweets'].apply(lambda x : TextBlob(x).correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
